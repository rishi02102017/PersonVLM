# Person VLM Configuration
# ========================
# Scaled-up Vision-Language Model for Person Description
# Optimized for V100 GPU training

# Model Architecture
model:
  # Vision encoder (mostly frozen)
  vision:
    backbone: "mobilevit_xs"  # Options: mobilevit_xxs, mobilevit_xs, efficientnet_lite0, vit_tiny_patch16_224
    pretrained: true
    freeze_ratio: 0.9  # Freeze 90% of vision encoder
  
  # Projection layer
  projection:
    num_visual_tokens: 8  # Number of visual tokens for decoder
    hidden_dim: 1024  # Scaled up for larger decoder
  
  # Text decoder
  decoder:
    size: "large"  # Options: tiny (~5M), small (~4M), medium (~10M), large (~20M)
  
  # Shared dimensions
  hidden_dim: 512  # Scaled up from 256 to 512
  
  # Regularization
  dropout: 0.1
  label_smoothing: 0.1

# Training Configuration (Optimized for V100 GPUs)
# 
# IMPORTANT: Linear Scaling Rule (Goyal et al., 2017)
# When batch size increases, LR should scale proportionally.
# Baseline: batch=32, LR=1e-4 → Scaled: batch=256, LR should be ~2e-4
#
training:
  # Basic settings
  epochs: 75  # Larger models need more epochs to converge
  batch_size: 64  # Per-GPU (effective = 64 × 4 = 256)
  
  # Optimizer
  optimizer: "adamw"
  learning_rate: 2.0e-4  # Scaled up per Linear Scaling Rule (was 5e-5, too low!)
  weight_decay: 0.005  # Reduced for larger model (was 0.01)
  
  # Scheduler
  scheduler: "cosine"
  warmup_ratio: 0.05  # Shorter warmup (5%) to reach peak LR faster
  
  # Mixed precision (enabled for CUDA)
  use_amp: true
  
  # Gradient clipping
  gradient_clip: 1.0
  
  # Data loading
  num_workers: 8  # Increased for faster data loading
  image_size: 224
  max_seq_length: 256  # Accommodate detailed MSP60k captions (mean ~100 tokens)
  
  # Checkpointing
  output_dir: "./checkpoints_scaled"  # Overwrite previous suboptimal training
  save_every: 1  # epochs
  
  # Early stopping
  early_stopping: true
  patience: 15  # Much more patience - larger models converge slowly
  
  # Logging
  log_every: 50  # steps
  eval_every: 500  # steps
  
  # Reproducibility
  seed: 42

# Data Configuration
data:
  # Dataset paths (MSP60k format)
  train_file: "PERSON_DATA/caption_with_attribute_labels/train.jsonl"
  val_file: "PERSON_DATA/caption_with_attribute_labels/val.jsonl"
  
  # Image directory (where person blob images are stored)
  image_dir: "PERSON_DATA/images"
  
  # Original dataset file (before split)
  full_dataset: "PERSON_DATA/caption_with_attribute_labels/MSP60k_train_v2.jsonl"
  
  # Augmentation
  augment_train: true
  
  # Vocabulary
  vocab_file: "data/vocabulary.json"  # Built from MSP60k corpus
  max_vocab_size: 5000  # Upper bound for vocabulary building
  
  # Split ratios (if creating splits)
  val_ratio: 0.1
  test_ratio: 0.05

# Inference Configuration
inference:
  # Generation parameters
  temperature: 0.7
  top_k: 50
  top_p: 0.9
  max_length: 64
  
  # Batch processing
  batch_size: 16
  
  # Confidence estimation
  num_samples: 5  # For confidence estimation

# Deployment
deployment:
  # ONNX export
  onnx:
    enable: false
    output_path: "model.onnx"
    opset_version: 14
  
  # Quantization (optional)
  quantization:
    enable: false
    type: "dynamic"  # Options: dynamic, static

# Hardware Configuration
hardware:
  device: "auto"  # auto, cuda, mps, cpu
  precision: "fp16"  # fp16, fp32, bf16
  
# Multi-GPU Configuration (DDP)
distributed:
  # Automatically detected at runtime
  enabled: true
  backend: "nccl"  # NCCL is optimized for NVIDIA GPUs
  
# V100-Specific Optimizations
# ============================
# The Tesla V100 has unique capabilities:
#
# 1. TENSOR CORES: 
#    - FP16 Tensor Core ops: 125 TFLOPS
#    - FP32 ops: 15.7 TFLOPS  
#    - Using AMP (mixed precision) gives ~8x speedup
#
# 2. HBM2 MEMORY:
#    - 32GB per GPU with 900 GB/s bandwidth
#    - Allows larger batch sizes (64-128 per GPU)
#
# 3. NVLINK:
#    - 300 GB/s bidirectional bandwidth between GPUs
#    - Enables efficient gradient synchronization in DDP
#
# 4. NCCL BACKEND:
#    - Collective communication optimized for NVIDIA GPUs
#    - Uses NVLink when available, falls back to PCIe

# Recommended Configurations for Different Budgets
# ================================================
#
# 1. BASELINE (~7M params, original)
#    model:
#      hidden_dim: 256
#      decoder:
#        size: "small"
#    training:
#      batch_size: 32
#
# 2. SCALED (~25M params, current configuration)
#    [Current default - optimized for V100]
#    model:
#      hidden_dim: 512
#      decoder:
#        size: "large"  # 6 layers, 512 dim, 2048 FFN
#    training:
#      batch_size: 64
#      learning_rate: 5.0e-5
#
# 3. MAXIMUM (~40-50M params, if needed)
#    model:
#      hidden_dim: 768
#      decoder:
#        size: "large" (modify to 8 layers)
#    training:
#      batch_size: 32
#      learning_rate: 3.0e-5
